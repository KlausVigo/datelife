---
title: "First benchmarking report"
author: Luna L. Sanchez Reyes
output: html_document
---
```{r setup, include = FALSE, warnings = FALSE}

library(drake)
library(microbenchmark)
library(testthat)
library(knitr)
knitr::opts_knit$set(root.dir = "~/Desktop/datelife/data-raw/benchmark/runtime_tests/")
ninput = c(10,100,200,300,400,500,700,1000,1500,2000,3000,5000,7000,8000,9000,10000)
```
# Main `datelife` function benchmarking.

We used aves species names to benchmark. First, we used the same character vector of species names for each test.

We ran `datelife_search`, previously called `get_filtered_results` with different number of input taxa: `r paste0(ninput, collapse = ", ")`
and with all known bird species.
```{r, echo=FALSE}
	res = c()
	getwd()
	for(i in ninput){
		xname = paste0("aves",i,".1.gfr.runtime_2017.12.29")
		x = paste0("2_tests/1_same_spp_names/", xname, ".RData")
		load(x)
		res = rbind(res, get(xname))
	}
	# expect_true(length(res) == 2)
	# expect_true(inherits(res, "microbenchmark"))
	load("2_tests/0_all_names/aves.all.gfr.runtime_2017.12.29.RData")
	# expect_true(inherits(aves.all.gfr.runtime_2017.12.29, "microbenchmark"))
	resf = rbind(res, aves.all.gfr.runtime_2017.12.29)
	plt = microbenchmark:::autoplot.microbenchmark(resf)
	plt
```

More:

- Walkthrough: [this chapter of the user manual](https://ropenscilabs.github.io/drake-manual/intro.html)
- Slides: [https://krlmlr.github.io/drake-pitch](https://krlmlr.github.io/drake-pitch)
- Code: `drake_example("main")`
